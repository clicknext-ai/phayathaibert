[training]
num_epochs = 4
batch_size = 8
# Can be "no", "epoch", "once" or int for raw steps
eval_steps = 100
save_steps = 100
gradient_accumulation_steps = 32
mixed_precision = "fp16"

[optimizer]
peak_lr = 3e-4
weight_decay = 0.01
eps = 1e-6
betas = [0.9, 0.99]
# Comment this out to disable 'discriminative fine-tuning'
layer_lr_decay_factor = 2.6

[scheduler]
type = "linear"
num_warmup_steps = 24000
max_steps = 500000

# Comment this table out to disable 'gradual unfreezing'
[unfreezing]
# Can be "epoch" or "step"
mode = "step"
# schedule must have length equal to the number of layers
schedule = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000]

# Must be exhaustive, Ordered from "last" to "first"
# Used as grouping for 'gradual unfreezing' and 'discriminative fine-tuning'
# Weights from previous layers are automatically excluded to avoid overlap
[[layer]]
include = ["roberta.embeddings.new_word_embeddings.weight", "lm_head.new_bias"]
[[layer]]
include = ["lm_head"]
exclude = ["lm_head.old_decoder.weight"]
[[layer]]
include = ["roberta.encoder.layer.11"]
[[layer]]
include = ["roberta.encoder.layer.10"]
[[layer]]
include = ["roberta.encoder.layer.9"]
[[layer]]
include = ["roberta.encoder.layer.8"]
[[layer]]
include = ["roberta.encoder.layer.7"]
[[layer]]
include = ["roberta.encoder.layer.6"]
[[layer]]
include = ["roberta.encoder.layer.5"]
[[layer]]
include = ["roberta.encoder.layer.4"]
[[layer]]
include = ["roberta.encoder.layer.3"]
[[layer]]
include = ["roberta.encoder.layer.2"]
[[layer]]
include = ["roberta.encoder.layer.1"]
[[layer]]
include = ["roberta.encoder.layer.0"]
[[layer]]
include = ["roberta.embeddings"]
